# -*- coding: utf-8 -*-
"""Enrollment Forecasting: Predict future enrollment numbers to optimize resource allocation and course offerings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQEFKIgW_BGh866Z_M26KYNJW99PnTLm
"""

pip install faker

import random
import csv

# Define the column headers
column_headers = [
    'Time_Period', 'Course_Name', 'Course_Code', 'Department',
    'Student_Demographics', 'Course_Attributes', 'Enrollment_Numbers',
    'Historical_Data', 'External_Factors', 'Marketing_Data', 'Waitlist_Data',
    'Registration_Period', 'Tuition_Data', 'Retention_Data', 'Survey_Feedback'
]

# Initialize the Faker library
fake = Faker()

# Define some sample data for each column
time_periods = ['Fall 2023', 'Spring 2024', 'Summer 2024']
course_names = ['Introduction to Computer Science', 'History of Art', 'Statistics 101']
course_codes = ['CS101', 'ART200', 'STAT101']
departments = ['Computer Science', 'Art History', 'Mathematics']
student_demographics = ['Undergraduate', 'Graduate']
course_attributes = ['Core', 'Elective']
enrollment_numbers = [random.randint(10, 100) for _ in range(3)]
historical_data = ['None', 'Low', 'High']
external_factors = ['Economic downturn', 'Government funding increase', 'None']
marketing_data = ['Online ads', 'Social media campaigns', 'Word of mouth']
waitlist_data = [random.randint(0, 20) for _ in range(3)]
registration_period = ['2 weeks', '4 weeks', '6 weeks']
tuition_data = [round(random.uniform(1000, 5000), 2) for _ in range(3)]
retention_data = ['Low', 'Medium', 'High']
survey_feedback = ['Positive', 'Neutral', 'Negative']

# Generate and write 10 rows of fake data to a CSV file
with open('fake_data.csv', 'w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the column headers
    writer.writerow(column_headers)

    for _ in range(10):
        time_period = random.choice(time_periods)
        course_name = random.choice(course_names)
        course_code = random.choice(course_codes)
        department = random.choice(departments)
        student_demo = random.choice(student_demographics)
        course_attr = random.choice(course_attributes)
        enrollment = random.choice(enrollment_numbers)
        history_data = random.choice(historical_data)
        ext_factors = random.choice(external_factors)
        marketing = random.choice(marketing_data)
        waitlist = random.choice(waitlist_data)
        reg_period = random.choice(registration_period)
        tuition = round(random.choice(tuition_data), 2)
        retention = random.choice(retention_data)
        survey_fb = random.choice(survey_feedback)

        data_row = [
            time_period, course_name, course_code, department, student_demo,
            course_attr, enrollment, history_data, ext_factors, marketing,
            waitlist, reg_period, tuition, retention, survey_fb
        ]

        # Write the data row
        writer.writerow(data_row)

pip install sweetviz

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import sweetviz as sv
# Load the dataset into a Pandas DataFrame
df = pd.read_csv('/content/fake_data.csv')
# Create the EDA report
report = sv.analyze(df)

# Show the report in a Jupyter Notebook (if you're using Jupyter)
report.show_html('eda_report.html')

df.head()

# Define the mapping dictionaries for each column
course_codes_mapping = {code: index for index, code in enumerate(course_codes)}
departments_mapping = {dept: index for index, dept in enumerate(departments)}
student_demographics_mapping = {demo: index for index, demo in enumerate(student_demographics)}
course_attributes_mapping = {attr: index for index, attr in enumerate(course_attributes)}
historical_data_mapping = {data: index for index, data in enumerate(historical_data)}
external_factors_mapping = {factor: index for index, factor in enumerate(external_factors)}
marketing_data_mapping = {data: index for index, data in enumerate(marketing_data)}
registration_period_mapping = {period: index for index, period in enumerate(registration_period)}
retention_data_mapping = {data: index for index, data in enumerate(retention_data)}
survey_feedback_mapping = {feedback: index for index, feedback in enumerate(survey_feedback)}

# Map the values in the DataFrame
df['Course_Code'] = df['Course_Code'].map(course_codes_mapping)
df['Department'] = df['Department'].map(departments_mapping)
df['Student_Demographics'] = df['Student_Demographics'].map(student_demographics_mapping)
df['Course_Attributes'] = df['Course_Attributes'].map(course_attributes_mapping)
df['Historical_Data'] = df['Historical_Data'].map(historical_data_mapping)
df['External_Factors'] = df['External_Factors'].map(external_factors_mapping)
df['Marketing_Data'] = df['Marketing_Data'].map(marketing_data_mapping)
df['Registration_Period'] = df['Registration_Period'].map(registration_period_mapping)
df['Retention_Data'] = df['Retention_Data'].map(retention_data_mapping)
df['Survey_Feedback'] = df['Survey_Feedback'].map(survey_feedback_mapping)

# Apply one-hot encoding to the 'Course_Code' column
df = pd.get_dummies(df, columns=['Course_Name'])
df = pd.get_dummies(df, columns=['Time_Period'])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_error
from math import sqrt

# Split the dataset into features (X) and the target variable (y)
X = df.drop(columns=['Enrollment_Numbers'])
y = df['Enrollment_Numbers']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest model
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)

print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
# Add a new column to the DataFrame with the predicted results
X_test['Predicted_Enrollment'] = y_pred

# Print the DataFrame with the added column
print(X_test)

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt

# Split the dataset into features (X) and the target variable (y)
X = df.drop(columns=['Enrollment_Numbers'])
y = df['Enrollment_Numbers']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train an XGBoost model
model = XGBRegressor(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)

print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

"""**Conclusion**

performance metrics, the Random Forest model appears to be a better choice for predicting enrollment numbers in your dataset. It generally provides more accurate and precise predictions. However, you should consider other factors like model interpretability, ease of deployment, and computational resources when choosing the best model for your specific application.
"""

